# ------------------------------------------------------------------------------
# RAG Chatbot with Gradio Interface
# ------------------------------------------------------------------------------
# Author: Tirth Shah
# Description: A Retrieval-Augmented Generation (RAG) chatbot using completely
#              free local models. Uses Ollama for LLM inference and HuggingFace
#              for embeddings. No API keys or external costs required.
#
# Features:
# - File upload and processing separation for better UX
# - State management for efficient question answering
# - Free local inference with Ollama (llama2)
# - HuggingFace embeddings with no API requirements
# - Clean Gradio interface with real-time status updates
#
# Setup: See README.md for platform-specific installation steps
# ------------------------------------------------------------------------------

from typing import Tuple, Optional
import gradio as gr
import warnings

# Import RAG functions from separate module
from rag_functions import get_llm, retriever
from langchain.chains import RetrievalQA


def warn(*args, **kwargs) -> None:
    """Suppress warnings."""
    pass


# You can use this section to suppress warnings generated by your code:
warnings.warn = warn
warnings.filterwarnings('ignore')


def process_file(file) -> Tuple[Optional[RetrievalQA], str]:
    """
    Process uploaded file and create QA system.

    Parameters
    ----------
    file : object or None
        Gradio file object with name attribute, or None if no file uploaded.

    Returns
    -------
    Tuple[Optional[RetrievalQA], str]
        Tuple containing the QA system (or None if failed) and status message.
    """

    if file is None:
        return None, "Please upload a PDF file first."
    
    try:
        llm = get_llm()
        retriever_obj = retriever(file)
        qa = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever_obj,
            return_source_documents=False
        )
        return qa, f"✅ File '{file.name}' processed successfully! You can now ask questions."
    except Exception as e:
        return None, f"❌ Error processing file: {str(e)}"


def answer_question(qa_system: Optional[RetrievalQA], query: str) -> str:
    """
    Answer question using the processed QA system.

    Parameters
    ----------
    qa_system : Optional[RetrievalQA]
        The QA system created from processing a file, or None if no file processed.
    query : str
        The question to ask about the document.

    Returns
    -------
    str
        The answer to the question or error message.
    """
    
    if qa_system is None:
        return "Please upload and process a PDF file first."
    
    if not query.strip():
        return "Please enter a question."
    
    try:
        response = qa_system.invoke(query)
        return response['result']
    except Exception as e:
        return f"❌ Error answering question: {str(e)}"

# Create Gradio interface with state management
with gr.Blocks(title="RAG Chatbot") as rag_application:
    gr.Markdown("# RAG Chatbot")
    gr.Markdown("Upload a PDF document and ask any question. The chatbot will try to answer using the provided document.")
    
    # State to store the QA system
    qa_state = gr.State(value=None)
    
    with gr.Row():
        with gr.Column():
            file_input = gr.File(
                label="Upload PDF File", 
                file_count="single", 
                file_types=['.pdf'], 
                type="filepath"
            )
            status_output = gr.Textbox(
                label="Status", 
                interactive=False,
                placeholder="Upload a PDF file to get started..."
            )
        
        with gr.Column():
            query_input = gr.Textbox(
                label="Input Query", 
                lines=2, 
                placeholder="Type your question here..."
            )
            submit_btn = gr.Button("Submit Question", variant="primary")
            answer_output = gr.Textbox(
                label="Answer", 
                lines=5
            )
    
    # Event handlers
    file_input.change(
        fn=process_file,
        inputs=[file_input],
        outputs=[qa_state, status_output]
    )
    
    submit_btn.click(
        fn=answer_question,
        inputs=[qa_state, query_input],
        outputs=[answer_output]
    )

# Launch the app
if __name__ == "__main__":
    rag_application.launch(server_name="0.0.0.0", server_port=7860)